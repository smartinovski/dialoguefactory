{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f5d8249",
   "metadata": {},
   "source": [
    "## Introduction to the baseline\n",
    "This notebook shows an example of training a GRU Seq2Seq model on our dialogue data using supervised learning. We also use it as a baseline in our challenge. You can also find this baseline helpful if you want to train or evaluate a reinforcement learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5765a9d0",
   "metadata": {},
   "source": [
    "### Import the necessary packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6048482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from dialoguefactory.environments import easy\n",
    "from dialoguefactory.environments import hard\n",
    "\n",
    "import dialoguefactory.trainers.vocab as vocab\n",
    "from dialoguefactory.generation import mappers_database as mdb\n",
    "import dialoguefactory.dialogue_generator as dg\n",
    "\n",
    "import dialoguefactory.trainers.arch as arch\n",
    "import dialoguefactory.trainers.baseline as baseline\n",
    "import dialoguefactory.trainers.evaluation as evl\n",
    "\n",
    "home_directory = os.path.expanduser('~')\n",
    "\n",
    "error_path = os.path.join(home_directory, 'dialoguefactory_logs', 'error.log')\n",
    "context_path = os.path.join(home_directory, 'dialoguefactory_logs', 'context.log')\n",
    "os.makedirs(os.path.dirname(error_path), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(context_path), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ce7397",
   "metadata": {},
   "source": [
    "### Initialize the dialogue generator and the environments\n",
    "\n",
    "We create the training environment that we call _easy_and generate the training dialogues in it. The _hard_ environment is an extension of the easy environment, and we use it to see how well the agent generalizes in new environments. Later, we merge the new hard environment with the easy one and evaluate the model in the extended environment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7640a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_world = easy.build_world()\n",
    "hard_world = hard.build_world()\n",
    "database = mdb.create_database_all_mappers()\n",
    "dia_generator = dg.DialogueGenerator(easy_world, error_path, context_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3f4538",
   "metadata": {},
   "source": [
    "We choose the number of training points and the main agent to be trained. Only one agent is trained, and we selected Gretel as our main agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c26d50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 175000\n",
    "train_num_points = int (num_points*0.85)\n",
    "val_num_points = num_points - train_num_points\n",
    "main_player = easy_world.player\n",
    "other_players = [p for p in easy_world.players if p != main_player]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07b446c",
   "metadata": {},
   "source": [
    "### Generate and prepare the data for training\n",
    "\n",
    "The dialogues consist of a user issuing a request to an agent.\n",
    "\n",
    "We generate the dialogues and prepare the data in the following format: (unseen_context, agent_response). \n",
    "We expect the main agent to provide a response in the dialogue based on the seen context. The unseen_context consists of sentences from the current dialogue or previous dialogues that the agent has yet to see (if a new dialogue has started). The state of the GRU model is continuous; that's why we don't use the full context as input.\n",
    "\n",
    "In the simulation, we allow the generation of dialogues between all agents in the environment. Because there are five agents in the training environment, the main agent participates in 20% of the dialogues and has to respond to a user issuing a request. In the rest, the other agents communicate with each other. This way, the main agent can learn from the actions of the other agents in the simulated world. In the simulation we do not generate dialogues where the main agent plays the user role.\n",
    "\n",
    "Since the simulation is continual, we preserve the continuity of the data when generating the train, val, and test data. We do it by providing the last_context_id, the index of the last context sentence observed by the main agent.  \n",
    "Inside the generate_data function, we serialize the output sentences of the main agent into a format explained in the function [serialize](https://revivegretel.com/docs/dialoguefactory.trainers.html#dialoguefactory.trainers.serializers.serialize). We represent each sentence's meaning using a [Describer](https://revivegretel.com/docs/dialoguefactory.language.html#dialoguefactory.language.components.Describer). Therefore, the process of serialization is converting the Describer's arguments in a list of strings/tokens. During the serialization, we do not allow representing the entities in the world using their unique names (var_name). For example, when we serialize the entity Gretel, the 'name' property can be used: \\['bentity', 'Gretel', 'eentity'\\]. But it is not allowed to serialize the following way: \\['bentity', 'player', 'eentity'\\] or ['bentity', 'main', 'player', 'eentity'\\]. The 'var_name' property and the ('main', 'player') attribute are not allowed in the output because we want to test whether the agent can identify the entities using properties and attributes. \n",
    "\n",
    "During testing, we deserialize the list of tokens into a class Sentence using the function [deserialize](https://revivegretel.com/docs/dialoguefactory.trainers.html#dialoguefactory.trainers.serializers.deserialize). Feel free to use any serializing/deserializing method as long as the output sentence contains the correct Describer and does not use the.\n",
    "\n",
    "Please note that we only use the hard environment in the following cell to fetch the words needed for the input/output vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e24fdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data_x, train_data_y, last_context_id, num_train_dias = baseline.generate_data(dia_generator, \n",
    "                                                                                      train_num_points, \n",
    "                                                                                      main_player, \n",
    "                                                                                      other_players, \n",
    "                                                                                      0, \n",
    "                                                                                      100, \n",
    "                                                                                      True)\n",
    "val_data_x, val_data_y, last_context_id, num_val_dias = baseline.generate_data(dia_generator, \n",
    "                                                                                val_num_points, \n",
    "                                                                                main_player,\n",
    "                                                                                other_players, \n",
    "                                                                                last_context_id, \n",
    "                                                                                100, \n",
    "                                                                                True)\n",
    "\n",
    "data_x = train_data_x+val_data_x\n",
    "data_y = train_data_y+val_data_y\n",
    "orig_data_y = data_y\n",
    "\n",
    "input_voc = vocab.Vocabulary(vocab.compute_input_vocab(easy_world, hard_world)+['bos','eos'],'bos','eos')\n",
    "output_voc = vocab.Vocabulary(vocab.compute_output_vocab(easy_world, hard_world)+['bos', 'eos'], 'bos', 'eos')\n",
    "\n",
    "\n",
    "data_x = input_voc.to_indices(data_x)\n",
    "data_y = output_voc.to_indices(data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3244b63e",
   "metadata": {},
   "source": [
    "### \n",
    "\n",
    "Split the preprocessed data and configure the Pytorch model\n",
    "\n",
    "We use batch_size = 1 since the Seq2Seq model is continuous and can not run multiple input samples in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd1f5b6-364c-43e3-9ecd-0cf4d829e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data_x, train_data_y),  (val_data_x, val_data_y) = baseline.dataset_split(data_x, data_y, len(train_data_y))\n",
    "\n",
    "max_len = max(map(len, data_y))\n",
    "model = arch.Seq2SeqContModel(input_size = len(input_voc),\n",
    "                     embed_size = 32,\n",
    "                     encoder_hidden_size = 128,\n",
    "                     decoder_hidden_size = 128,\n",
    "                     output_size = len(output_voc),\n",
    "                     num_layers = 1)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb869258",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "We train the model using backpropagation through time (BPTT) and save the encoder's state for the next iteration. \n",
    "We do not shuffle the data since the model is continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25053ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "num_epochs = 1\n",
    "for e in range(num_epochs):\n",
    "    enc_hid_state = None\n",
    "    progress_bar = tqdm(total=int(train_num_points/batch_size))\n",
    "    for bx, by in arch.generate_batch(train_data_x,train_data_y,batch_size, shuffle=False):\n",
    "        train_loss, enc_hid_state = arch.compute_loss(model, bx, by, input_voc, output_voc, max_len, enc_hid_state)\n",
    "        train_loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        enc_hid_state = enc_hid_state.detach()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    for bx, by in arch.generate_batch(val_data_x, val_data_y, batch_size, shuffle=False):\n",
    "        val_loss, enc_hid_state = arch.compute_loss(model, bx, by, input_voc, output_voc, max_len, enc_hid_state)\n",
    "        enc_hid_state = enc_hid_state.detach()\n",
    "\n",
    "    print (train_loss.item(), val_loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d524dcd8",
   "metadata": {},
   "source": [
    "### Evaluating the model\n",
    "\n",
    "We evaluate the model in the easy environment and print all the metrics for the leaderboard. The evaluation for both the training and testing environment must be done on at least 200000 dialogues in which the main agent participates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96842a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dialoguefactory.trainers.evaluation import pretty_print_eval\n",
    "new_policy = baseline.AgentPolicy(main_player, database, model, enc_hid_state, last_context_id, input_voc, output_voc, max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4894bb-3ff9-40b2-a28f-9cf55b8607c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dias, individual_accuracies, total_accuracy, num_agent_dias = evl.generate_and_eval (dia_generator, 1, 200000, new_policy, 100,  notebook_run=True)\n",
    "\n",
    "pretty_print_eval(\"baseline easy\", individual_accuracies, total_accuracy, num_agent_dias, num_train_dias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133c2b6f",
   "metadata": {},
   "source": [
    "We evaluate the model in the extended environment (easy+hard). To do the evaluation, we first merge the easy environment with the hard one to preserve continuity. We unlock the locked doors in the easy environment that lead to the hard environment. We inject the information that the doors are no longer locked in the context so the agent can observe this information.\n",
    "\n",
    "When evaluating, we chose to have our main player play the role of an agent in 20% of the dialogues, similar to the training environment. We did not set this parameter to 0.2 in the training environment because there are five players in the training environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348ee8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard.merge_worlds(dia_generator, hard_world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdcb32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dias, individual_accuracies, total_accuracy, num_agent_dias = evl.generate_and_eval (dia_generator, 1, 200000, new_policy, 100, 0.2, notebook_run=True)\n",
    "\n",
    "pretty_print_eval(\"baseline hard\", individual_accuracies, total_accuracy, num_agent_dias, num_train_dias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
